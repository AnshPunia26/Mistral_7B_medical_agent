{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Mistral 7B Medical Q&A Fine-tuning on Google Colab\n",
    "\n",
    "This notebook fine-tunes a Mistral 7B model on medical question-answering data using LoRA (Low-Rank Adaptation) and automatically pushes it to Hugging Face Hub. Optimized for A100 GPU.\n",
    "\n",
    "## Setup Instructions:\n",
    "1. **Get Hugging Face Token**: Visit https://huggingface.co/settings/tokens and create a token with \"write\" permissions\n",
    "2. **Enable GPU**: Runtime \u2192 Change runtime type \u2192 GPU (A100 recommended, T4 or better)\n",
    "3. Upload your `mistral_fine_tune_format.jsonl` file to Colab (or use the provided upload cell)\n",
    "4. Run all cells in order\n",
    "5. The fine-tuned model will be automatically pushed to your Hugging Face account\n",
    "\n",
    "## What You'll Need:\n",
    "- Hugging Face account (free at https://huggingface.co)\n",
    "- Hugging Face token with write permissions\n",
    "- Your `mistral_fine_tune_format.jsonl` data file (already in Mistral chat format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Install Dependencies and Setup Hugging Face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install -q transformers datasets accelerate peft bitsandbytes sentencepiece tensorboard huggingface_hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Login to Hugging Face\n",
    "\n",
    "You need a Hugging Face token to push the model. Get one from: https://huggingface.co/settings/tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from huggingface_hub import login, HfApi\n",
    "import getpass\n",
    "\n",
    "# Login to Hugging Face\n",
    "print(\"Please enter your Hugging Face token:\")\n",
    "print(\"Get your token from: https://huggingface.co/settings/tokens\")\n",
    "hf_token = getpass.getpass(\"Hugging Face Token: \")\n",
    "\n",
    "# Login\n",
    "login(token=hf_token)\n",
    "print(\"\u2713 Successfully logged in to Hugging Face!\")\n",
    "\n",
    "# Set your Hugging Face username (for model repository name)\n",
    "HF_USERNAME = input(\"Enter your Hugging Face username: \").strip()\n",
    "MODEL_REPO_NAME = f\"{HF_USERNAME}/mistral-7b-medical-qa-finetuned\"\n",
    "print(f\"Model will be pushed to: {MODEL_REPO_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Upload Data File\n",
    "\n",
    "Upload your `mistral_fine_tune_format.jsonl` file. If you already have it in your Google Drive, you can mount Drive instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Upload file\n",
    "print(\"Please upload your mistral_fine_tune_format.jsonl file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move to data directory if needed\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.jsonl'):\n",
    "        os.rename(filename, f'data/{filename}')\n",
    "        print(f\"\u2713 File saved to: data/{filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Fine-tuning Configuration\n",
    "\n",
    "**Note:** The dataset should already be in Mistral format (messages with user/assistant roles). If you need to convert data, use the conversion script first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Configuration optimized for A100 GPU\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"  # Mistral 7B Instruct model\n",
    "\n",
    "# Find the uploaded file (should be mistral_fine_tune_format.jsonl)\n",
    "from pathlib import Path\n",
    "data_files = list(Path('data').glob('*.jsonl'))\n",
    "if not data_files:\n",
    "    data_files = list(Path('.').glob('*.jsonl'))\n",
    "\n",
    "if data_files:\n",
    "    DATA_PATH = str(data_files[0])\n",
    "    print(f\"\u2713 Found dataset: {DATA_PATH}\")\n",
    "else:\n",
    "    DATA_PATH = \"data/mistral_fine_tune_format.jsonl\"\n",
    "    print(f\"\u26a0 Dataset not found, will use: {DATA_PATH}\")\n",
    "\n",
    "OUTPUT_DIR = \"./mistral_medical_finetuned\"\n",
    "MAX_SEQ_LENGTH = 2048  # Larger context for A100 GPU\n",
    "BATCH_SIZE = 8  # Larger batch size for A100 (can go up to 16-32)\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch size = 8 * 4 = 32\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5  # Slightly lower for larger model\n",
    "\n",
    "# LoRA configuration (Medium settings)\n",
    "LORA_R = 64  # Medium rank (was 128 high)\n",
    "LORA_ALPHA = 128  # Typically 2x the rank (was 256 high)\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Load Model and Tokenizer\n",
    "\n",
    "**Note:** Make sure you've logged in to Hugging Face in Step 1 before proceeding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"Using 4-bit quantization for memory efficiency...\")\n",
    "\n",
    "# Configure tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\",\n",
    ")\n",
    "\n",
    "# Set pad token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Use bfloat16 on A100 for better stability\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "print(\"Loading model with 4-bit quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Automatically place on GPU\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 on A100\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(\"\u2713 Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Setup LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Setting up LoRA for efficient fine-tuning...\")\n",
    "\n",
    "# Get target modules for Mistral model\n",
    "target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\u2713 LoRA setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 6: Define Helper Functions for Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer):\n",
    "    \"\"\"Preprocess the dataset for training using Mistral chat template.\"\"\"\n",
    "    # Format prompts from messages using tokenizer's chat template\n",
    "    prompts = []\n",
    "    for messages_list in examples[\"messages\"]:\n",
    "        # Use the tokenizer's chat template to format messages\n",
    "        if hasattr(tokenizer, 'apply_chat_template') and tokenizer.chat_template:\n",
    "            formatted = tokenizer.apply_chat_template(\n",
    "                messages_list, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "        else:\n",
    "            # Fallback: Manual formatting for Mistral\n",
    "            # Mistral format: [INST] instruction [/INST] response\n",
    "            if len(messages_list) >= 2:\n",
    "                user_msg = messages_list[0].get('content', '')\n",
    "                assistant_msg = messages_list[1].get('content', '')\n",
    "                formatted = f\"[INST] {user_msg} [/INST] {assistant_msg}\"\n",
    "            else:\n",
    "                formatted = \"\"\n",
    "        prompts.append(formatted)\n",
    "    \n",
    "    # Tokenize with padding=False (we'll pad in the collator)\n",
    "    # DataCollatorForLanguageModeling will automatically create labels from input_ids\n",
    "    model_inputs = tokenizer(\n",
    "        prompts,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        return_tensors=None,  # Return Python lists, not tensors\n",
    "    )\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "def load_and_prepare_dataset(data_path: str, tokenizer):\n",
    "    \"\"\"Load and prepare the dataset.\"\"\"\n",
    "    print(f\"Loading dataset from {data_path}...\")\n",
    "    \n",
    "    # Load JSONL file\n",
    "    dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "    \n",
    "    print(f\"Dataset loaded: {len(dataset)} examples\")\n",
    "    \n",
    "    # Preprocess\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(\n",
    "        lambda examples: preprocess_function(examples, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "    \n",
    "    # Split into train/validation (90/10)\n",
    "    dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    \n",
    "    print(f\"Train examples: {len(dataset['train'])}\")\n",
    "    print(f\"Validation examples: {len(dataset['test'])}\")\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 7: Load and Prepare Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = load_and_prepare_dataset(DATA_PATH, tokenizer)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 8: Setup Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training arguments optimized for A100 GPU\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    bf16=True,  # Use bfloat16 on A100 for better performance and stability\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_total_limit=3,  # Keep last 3 checkpoints\n",
    "    dataloader_pin_memory=True,  # Faster data loading on GPU\n",
    "    gradient_checkpointing=True,  # Save memory on A100\n",
    "    optim=\"paged_adamw_8bit\",  # Use 8-bit optimizer for memory efficiency\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked LM\n",
    "    pad_to_multiple_of=8,  # Pad to multiple of 8 for efficiency\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\u2713 Training setup complete!\")\n",
    "print(f\"Total training steps: {len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) * NUM_EPOCHS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 9: Start Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save final model\n",
    "print(f\"\\nSaving final model to {OUTPUT_DIR}...\")\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training complete!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 10: Push Model to Hugging Face Hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "import os\n",
    "\n",
    "# Create repository on Hugging Face (if it doesn't exist)\n",
    "api = HfApi()\n",
    "try:\n",
    "    create_repo(\n",
    "        repo_id=MODEL_REPO_NAME,\n",
    "        token=hf_token,\n",
    "        private=False,  # Set to True if you want a private repo\n",
    "        repo_type=\"model\",\n",
    "        exist_ok=True,\n",
    "    )\n",
    "    print(f\"\u2713 Repository created/verified: {MODEL_REPO_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "\n",
    "# Push model to Hugging Face Hub\n",
    "print(f\"\\nPushing model to Hugging Face Hub: {MODEL_REPO_NAME}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Push the model\n",
    "api.upload_folder(\n",
    "    folder_path=OUTPUT_DIR,\n",
    "    repo_id=MODEL_REPO_NAME,\n",
    "    token=hf_token,\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2713 Model successfully pushed to Hugging Face!\")\n",
    "print(f\"View your model at: https://huggingface.co/{MODEL_REPO_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 11: Download Fine-tuned Model (Optional - Local Backup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Create zip file for local backup (optional)\n",
    "shutil.make_archive('mistral_medical_finetuned', 'zip', OUTPUT_DIR)\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download('mistral_medical_finetuned.zip')\n",
    "\n",
    "print(\"\u2713 Model downloaded as backup!\")\n",
    "print(f\"Note: Model is also available on Hugging Face at: https://huggingface.co/{MODEL_REPO_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 12: Test the Fine-tuned Model (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Option 1: Load from local directory\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "# )\n",
    "# model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Option 2: Load directly from Hugging Face Hub (recommended)\n",
    "print(f\"Loading model from Hugging Face: {MODEL_REPO_NAME}\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, MODEL_REPO_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_REPO_NAME)\n",
    "\n",
    "# Test with a medical question\n",
    "test_question = \"What is diabetes?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": test_question}\n",
    "]\n",
    "\n",
    "# Use tokenizer's chat template\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.7, do_sample=True)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Question:\", test_question)\n",
    "print(\"\\nResponse:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}